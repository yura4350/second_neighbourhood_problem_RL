{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea394056",
   "metadata": {},
   "source": [
    "# 1. Add imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard and scientific libraries\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle \n",
    "import time\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "# Import to use with SageMath\n",
    "from sage.all import *\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Graph Theory\n",
    "# import networkx as nx\n",
    "\n",
    "# High-Performace Computing\n",
    "from numba import njit\n",
    "\n",
    "# TensorFlow and Keras (Modern Imports)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb096e",
   "metadata": {},
   "source": [
    "# 2. Set all the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ec01f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20 # number of vertices in the graph. Only used in the reward function, irrelevant to the algorithm\n",
    "DECISIONS = int(N*(N-1)/2) # The length of the word we are generating\n",
    "\n",
    "LEARNING_RATE = 0.0001 # Increase this to make convergence faster, decrease if algorithm gets stuck in local optima too often\n",
    "n_sessions = 1000 # number of sessions per iteration\n",
    "percentile = 93 # top 100-X percentile that survives to the next iteration\n",
    "super_percentile = 94 #top 100-X percentile that survives to next iteration\n",
    "\n",
    "FIRST_LAYER_NEURONS = 128 #Number of neurons in the hidden layers.\n",
    "SECOND_LAYER_NEURONS = 64\n",
    "THIRD_LAYER_NEURONS = 4\n",
    "\n",
    "n_actions = 2 #The size of the alphabet. In this file we will assume this is 2. There are a few things we need to change when the alphabet size is larger,\n",
    "\t\t\t  #such as one-hot encoding the input, and using categorical_crossentropy as a loss function.\n",
    "\n",
    "observation_space = 2*DECISIONS #Leave this at 2*DECISIONS. The input vector will have size 2*DECISIONS, where the first DECISIONS letters encode our partial word (with zeros on\n",
    "\t\t\t\t\t\t  #the positions we haven't considered yet), and the next DECISIONS bits one-hot encode which letter we are considering now.\n",
    "\t\t\t\t\t\t  #So e.g. [0,1,0,0,   0,0,1,0] means we have the partial word 01 and we are considering the third letter now.\n",
    "\t\t\t\t\t\t  #Is there a better way to format the input to make it easier for the neural network to understand things?\n",
    "\n",
    "len_game = DECISIONS \n",
    "state_dim = (observation_space,)\n",
    "\n",
    "INF = 1000000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46f3f52",
   "metadata": {},
   "source": [
    "# 3. Model structure\n",
    "\n",
    "Model structure: a sequential network with three hidden layers, sigmoid activation in the output. I usually used relu activation in the hidden layers but play around to see what activation function and what optimizer works best.\n",
    "It is important that the loss is binary cross-entropy if alphabet size is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb8bc1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">48,768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m48,768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │           \u001b[38;5;34m260\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m5\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,289</span> (223.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,289\u001b[0m (223.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,289</span> (223.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,289\u001b[0m (223.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(FIRST_LAYER_NEURONS,  activation=\"relu\"))\n",
    "model.add(Dense(SECOND_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(THIRD_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.build((None, observation_space))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=SGD(learning_rate = LEARNING_RATE)) #Adam optimizer also works well, with lower learning rate\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f8664",
   "metadata": {},
   "source": [
    "# Reward function\n",
    "\n",
    "Input: a 0-1 vector of length DECISIONS. It represents the graph (or other object) you have created, by representing each edge as 0 or 1\n",
    "\n",
    "Output: the reward/score for your construction. See files in the *demos* folder for examples.\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(state): # receives a state from the generate_session function\n",
    "    \"\"\"\n",
    "    Score based on a weighted penalty system.\n",
    "    Heavily punishes vertices that fail the desired condition, creating a strong gradient.\n",
    "    \"\"\"\n",
    "    n = A.nrows()\n",
    "    if n == 0:\n",
    "        return -1\n",
    "\n",
    "    A_squared = A * A\n",
    "    A_reach_in_2 = matrix([[1 if x > 0 else 0 for x in row] for row in A_squared])\n",
    "    Npp_matrix = A_reach_in_2 - A\n",
    "    for i in range(n):\n",
    "        Npp_matrix[i, i] = 0\n",
    "\n",
    "    # Accumulates the total \"penalty\" of the graph. Our goal is to minimize it.\n",
    "    total_penalty = 0\n",
    "    penalty_multiplier = 100  # A vertex that fails is 100x worse than a vertex that succeeds. This parameter can be tuned - noticed, that works bad for lower values\n",
    "\n",
    "    for v_idx in range(n):\n",
    " \n",
    "        out_degree = 0\n",
    "        out_degree = sum(1 for x in A[v_idx] if x > 0)\n",
    "\n",
    "        size_second_neighborhood = 0\n",
    "        size_second_neighborhood = sum(1 for x in Npp_matrix[v_idx] if x > 0)\n",
    "        \n",
    "        diff = size_second_neighborhood - out_degree\n",
    "\n",
    "        # Apply the penalty logic\n",
    "        if diff < 0:\n",
    "            # This is a \"good\" vertex. Its difference is negative.\n",
    "            # Adding it to the penalty makes the total penalty lower (better).\n",
    "\n",
    "            # 1st option suggested (In this case large penalty multiplier to be used to offset how good of a counter-vetix the vertix is)\n",
    "            # total_penalty += diff\n",
    "\n",
    "            # 2nd option suggested (but we also should somehow care about how 'good' of a vertix a good vertix is\n",
    "            total_penalty -= 1\n",
    "\n",
    "            # 3rd option - no reward to actually find a counterexample?\n",
    "            # pass\n",
    "\n",
    "        else:\n",
    "            # This is a \"bad\" vertex. Its difference is 0 or positive.\n",
    "            # We add its difference to the penalty, multiplied by the penalty factor.\n",
    "            # We add 1 to the difference so that even a difference of 0 gets a penalty.\n",
    "            total_penalty += (diff + 1) * penalty_multiplier\n",
    "    \n",
    "    \"\"\"\n",
    "    IMPORTANT: Graphs with sinks automatically satisfy second neighborhood problem and create local minima.\n",
    "    Hence, we largely penalize sinks in O(n * m) - same time complexity as done before, so really no influence\n",
    "    \"\"\"\n",
    "\n",
    "    sink_penalty = 5000  # A large, fixed penalty for each sink found.\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Check if vertex i is a sink (its out-degree is 0).\n",
    "        # We use the manual for loop for summation to ensure compatibility.\n",
    "        row_sum = 0\n",
    "        for x in A[i, :]:\n",
    "            row_sum += x\n",
    "        \n",
    "        if row_sum == 0:\n",
    "            # If it's a sink, add the large penalty to this graph's total penalty.\n",
    "            total_penalty += sink_penalty\n",
    "\n",
    "    # The search tries to maximize the score, so we return the negative of the penalty.\n",
    "    # A lower penalty results in a higher score.\n",
    "    return float(-total_penalty)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_calc_score = njit()(calc_score) # Creating highly optimized, compiled version of calc_score function to make it run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d735f9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(n_sessions, actions,state_next,states,prob, step, total_score):\n",
    "\t\n",
    "\tfor i in range(n_sessions):\n",
    "\t\t\n",
    "\t\tif np.random.rand() < prob[i]:\n",
    "\t\t\taction = 1\n",
    "\t\telse:\n",
    "\t\t\taction = 0\n",
    "\t\tactions[i][step-1] = action\n",
    "\t\tstate_next[i] = states[i,:,step-1]\n",
    "\n",
    "\t\tif (action > 0):\n",
    "\t\t\tstate_next[i][step-1] = action\n",
    "\t\tstate_next[i][DECISIONS + step-1] = 0\n",
    "\t\tif (step < DECISIONS):\n",
    "\t\t\tstate_next[i][DECISIONS + step] = 1\t\t\t\n",
    "\t\t#calculate final score\n",
    "\t\tterminal = step == DECISIONS\n",
    "\t\tif terminal:\n",
    "\t\t\ttotal_score[i] = jitted_calc_score(state_next[i])\n",
    "\t\n",
    "\t\t# record sessions \n",
    "\t\tif not terminal:\n",
    "\t\t\tstates[i,:,step] = state_next[i]\n",
    "\t\t\n",
    "\treturn actions, state_next,states, total_score, terminal\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b131161",
   "metadata": {},
   "outputs": [],
   "source": [
    "jitted_play_game = njit()(play_game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aed6c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(agent, n_sessions, verbose = 1):\n",
    "\t\"\"\"\n",
    "\tPlay n_session games using agent neural network.\n",
    "\tTerminate when games finish \n",
    "\t\n",
    "\tCode inspired by https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "\t\"\"\"\n",
    "\tstates =  np.zeros([n_sessions, observation_space, len_game], dtype=int)\n",
    "\tactions = np.zeros([n_sessions, len_game], dtype = int)\n",
    "\tstate_next = np.zeros([n_sessions,observation_space], dtype = int)\n",
    "\tprob = np.zeros(n_sessions)\n",
    "\tstates[:,DECISIONS,0] = 1\n",
    "\tstep = 0\n",
    "\ttotal_score = np.zeros([n_sessions])\n",
    "\trecordsess_time = 0\n",
    "\tplay_time = 0\n",
    "\tscorecalc_time = 0\n",
    "\tpred_time = 0\n",
    "\twhile (True):\n",
    "\t\tstep += 1\t\t\n",
    "\t\ttic = time.time()\n",
    "\t\tprob = agent.predict(states[:,:,step-1], batch_size = n_sessions) \n",
    "\t\tpred_time += time.time()-tic\n",
    "\t\t\n",
    "\t\tfor i in range(n_sessions):\n",
    "\t\t\t\n",
    "\t\t\tif np.random.rand() < prob[i]:\n",
    "\t\t\t\taction = 1\n",
    "\t\t\telse:\n",
    "\t\t\t\taction = 0\n",
    "\t\t\tactions[i][step-1] = action\n",
    "\t\t\ttic = time.time()\n",
    "\t\t\tstate_next[i] = states[i,:,step-1]\n",
    "\t\t\tplay_time += time.time()-tic\n",
    "\t\t\tif (action > 0):\n",
    "\t\t\t\tstate_next[i][step-1] = action\t\t\n",
    "\t\t\tstate_next[i][DECISIONS + step-1] = 0\n",
    "\t\t\tif (step < DECISIONS):\n",
    "\t\t\t\tstate_next[i][DECISIONS + step] = 1\t\t\t\n",
    "\t\t\tterminal = step == DECISIONS\n",
    "\t\t\ttic = time.time()\n",
    "\t\t\tif terminal:\n",
    "\t\t\t\ttotal_score[i] = calc_score(state_next[i])\n",
    "\t\t\tscorecalc_time += time.time()-tic\n",
    "\t\t\ttic = time.time()\n",
    "\t\t\tif not terminal:\n",
    "\t\t\t\tstates[i,:,step] = state_next[i]\t\t\t\n",
    "\t\t\trecordsess_time += time.time()-tic\n",
    "\t\t\t\n",
    "\t\t\n",
    "\t\tif terminal:\n",
    "\t\t\tbreak\n",
    "\t#If you want, print out how much time each step has taken. This is useful to find the bottleneck in the program.\t\t\n",
    "\tif (verbose):\n",
    "\t\tprint(\"Predict: \"+str(pred_time)+\", play: \" + str(play_time) +\", scorecalc: \" + str(scorecalc_time) +\", recordsess: \" + str(recordsess_time))\n",
    "\treturn states, actions, total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40d333fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "\t\"\"\"\n",
    "\tSelect states and actions from games that have rewards >= percentile\n",
    "\t:param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "\t:param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "\t:param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "\t:returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\t\n",
    "\tThis function was mostly taken from https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "\tIf this function is the bottleneck, it can easily be sped up using numba\n",
    "\t\"\"\"\n",
    "\tcounter = n_sessions * (100.0 - percentile) / 100.0\n",
    "\treward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "\telite_states = []\n",
    "\telite_actions = []\n",
    "\telite_rewards = []\n",
    "\tfor i in range(len(states_batch)):\n",
    "\t\tif rewards_batch[i] >= reward_threshold-0.0000001:\t\t\n",
    "\t\t\tif (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "\t\t\t\tfor item in states_batch[i]:\n",
    "\t\t\t\t\telite_states.append(item.tolist())\n",
    "\t\t\t\tfor item in actions_batch[i]:\n",
    "\t\t\t\t\telite_actions.append(item)\t\t\t\n",
    "\t\t\tcounter -= 1\n",
    "\telite_states = np.array(elite_states, dtype = int)\t\n",
    "\telite_actions = np.array(elite_actions, dtype = int)\t\n",
    "\treturn elite_states, elite_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0820fa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_super_sessions(states_batch, actions_batch, rewards_batch, percentile=90):\n",
    "\t\"\"\"\n",
    "\tSelect all the sessions that will survive to the next generation\n",
    "\tSimilar to select_elites function\n",
    "\tIf this function is the bottleneck, it can easily be sped up using numba\n",
    "\t\"\"\"\n",
    "\t\n",
    "\tcounter = n_sessions * (100.0 - percentile) / 100.0\n",
    "\treward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "\tsuper_states = []\n",
    "\tsuper_actions = []\n",
    "\tsuper_rewards = []\n",
    "\tfor i in range(len(states_batch)):\n",
    "\t\tif rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "\t\t\tif (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "\t\t\t\tsuper_states.append(states_batch[i])\n",
    "\t\t\t\tsuper_actions.append(actions_batch[i])\n",
    "\t\t\t\tsuper_rewards.append(rewards_batch[i])\n",
    "\t\t\t\tcounter -= 1\n",
    "\tsuper_states = np.array(super_states, dtype = int)\n",
    "\tsuper_actions = np.array(super_actions, dtype = int)\n",
    "\tsuper_rewards = np.array(super_rewards)\n",
    "\treturn super_states, super_actions, super_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e84d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters\n",
    "super_states =  np.empty((0,len_game,observation_space), dtype = int)\n",
    "super_actions = np.array([], dtype = int)\n",
    "super_rewards = np.array([])\n",
    "sessgen_time = 0\n",
    "fit_time = 0\n",
    "score_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d778ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRand = random.randint(0,1000) #used in the filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d642e",
   "metadata": {},
   "source": [
    "# Actual Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000000): #1000000 generations should be plenty\n",
    "\t#generate new sessions\n",
    "\t#performance can be improved with joblib\n",
    "\ttic = time.time()\n",
    "\tsessions = generate_session(model,n_sessions,0) #change 0 to 1 to print out how much time each step in generate_session takes \n",
    "\tsessgen_time = time.time()-tic\n",
    "\ttic = time.time()\n",
    "\t\n",
    "\tstates_batch = np.array(sessions[0], dtype = int)\n",
    "\tactions_batch = np.array(sessions[1], dtype = int)\n",
    "\trewards_batch = np.array(sessions[2])\n",
    "\tstates_batch = np.transpose(states_batch,axes=[0,2,1])\n",
    "\t\n",
    "\tstates_batch = np.append(states_batch,super_states,axis=0)\n",
    "\n",
    "\tif i>0:\n",
    "\t\tactions_batch = np.append(actions_batch,np.array(super_actions),axis=0)\t\n",
    "\trewards_batch = np.append(rewards_batch,super_rewards)\n",
    "\t\t\n",
    "\trandomcomp_time = time.time()-tic \n",
    "\ttic = time.time()\n",
    "\n",
    "\telite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile=percentile) #pick the sessions to learn from\n",
    "\tselect1_time = time.time()-tic\n",
    "\n",
    "\ttic = time.time()\n",
    "\tsuper_sessions = select_super_sessions(states_batch, actions_batch, rewards_batch, percentile=super_percentile) #pick the sessions to survive\n",
    "\tselect2_time = time.time()-tic\n",
    "\t\n",
    "\ttic = time.time()\n",
    "\tsuper_sessions = [(super_sessions[0][i], super_sessions[1][i], super_sessions[2][i]) for i in range(len(super_sessions[2]))]\n",
    "\tsuper_sessions.sort(key=lambda super_sessions: super_sessions[2],reverse=True)\n",
    "\tselect3_time = time.time()-tic\n",
    "\t\n",
    "\ttic = time.time()\n",
    "\tmodel.fit(elite_states, elite_actions) #learn from the elite sessions\n",
    "\tfit_time = time.time()-tic\n",
    "\t\n",
    "\ttic = time.time()\n",
    "\t\n",
    "\tsuper_states = [super_sessions[i][0] for i in range(len(super_sessions))]\n",
    "\tsuper_actions = [super_sessions[i][1] for i in range(len(super_sessions))]\n",
    "\tsuper_rewards = [super_sessions[i][2] for i in range(len(super_sessions))]\n",
    "\t\n",
    "\trewards_batch.sort()\n",
    "\tmean_all_reward = np.mean(rewards_batch[-100:])\t\n",
    "\tmean_best_reward = np.mean(super_rewards)\t\n",
    "\n",
    "\tscore_time = time.time()-tic\n",
    "\t\n",
    "\tprint(\"\\n\" + str(i) +  \". Best individuals: \" + str(np.flip(np.sort(super_rewards))))\n",
    "\t\n",
    "\t#uncomment below line to print out how much time each step in this loop takes. \n",
    "\tprint(\t\"Mean reward: \" + str(mean_all_reward) + \"\\nSessgen: \" + str(sessgen_time) + \", other: \" + str(randomcomp_time) + \", select1: \" + str(select1_time) + \", select2: \" + str(select2_time) + \", select3: \" + str(select3_time) +  \", fit: \" + str(fit_time) + \", score: \" + str(score_time)) \n",
    "\t\n",
    "\t\n",
    "\tif (i%20 == 1): #Write all important info to files every 20 iterations\n",
    "\t\twith open('best_species_pickle_'+str(myRand)+'.txt', 'wb') as fp:\n",
    "\t\t\tpickle.dump(super_actions, fp)\n",
    "\t\twith open('best_species_txt_'+str(myRand)+'.txt', 'w') as f:\n",
    "\t\t\tfor item in super_actions:\n",
    "\t\t\t\tf.write(str(item))\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\t\twith open('best_species_rewards_'+str(myRand)+'.txt', 'w') as f:\n",
    "\t\t\tfor item in super_rewards:\n",
    "\t\t\t\tf.write(str(item))\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\t\twith open('best_100_rewards_'+str(myRand)+'.txt', 'a') as f:\n",
    "\t\t\tf.write(str(mean_all_reward)+\"\\n\")\n",
    "\t\twith open('best_elite_rewards_'+str(myRand)+'.txt', 'a') as f:\n",
    "\t\t\tf.write(str(mean_best_reward)+\"\\n\")\n",
    "\tif (i%200==2): # To create a timeline, like in Figure 3\n",
    "\t\twith open('best_species_timeline_txt_'+str(myRand)+'.txt', 'a') as f:\n",
    "\t\t\tf.write(str(super_actions[0]))\n",
    "\t\t\tf.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
